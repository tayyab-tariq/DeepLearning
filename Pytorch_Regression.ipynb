{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torchvision.datasets import MNIST\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python\\Python3.7.6\\lib\\site-packages\\torchvision\\datasets\\mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = MNIST(root='data/',download = True)\n",
    "print(len(dataset))\n",
    "test_dataset = MNIST(root='data/', train = False)\n",
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label:  9\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image,label = dataset[22]\n",
    "plt.imshow(image,cmap='gray')\n",
    "print('Label: ',label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "# MNIST dataset (images and labels)\n",
    "dataset = MNIST(root='data/', \n",
    "                train=True,\n",
    "                transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "train_ds, val_ds = random_split(dataset, [50000, 10000])\n",
    "len(train_ds) , len(val_ds)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(train_ds,batch_size,shuffle=True)\n",
    "val_loader = DataLoader(val_ds,batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "input_size = 28*28\n",
    "num_classes = 10\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    return torch.tensor(torch.sum(preds == labels).item()/len(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "                                        nn.Flatten(),\n",
    "                                        nn.Linear(input_size, 250),\n",
    "                                        nn.BatchNorm1d(250),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.Dropout(0.3),\n",
    "                                        nn.Linear(250, 50),\n",
    "                                        nn.BatchNorm1d(50),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.Dropout(0.3),\n",
    "                                        nn.Linear(50, num_classes),\n",
    "                                    )\n",
    "\n",
    "    def forward(self,xb):\n",
    "        out = xb.view(xb.size(0),-1)\n",
    "        return self.layers(out)\n",
    "\n",
    "    def training_step(self, batch):\n",
    "        images, labels = batch\n",
    "        out = self(images)                          #   Predictions\n",
    "        loss = F.cross_entropy(out, labels)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        images, labels = batch\n",
    "        out = self(images)                        \n",
    "        loss = F.cross_entropy(out, labels)         \n",
    "        acc = accuracy(out, labels)                 #   Accuracy\n",
    "        return {'val_loss':loss, 'val_acc': acc}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()\n",
    "        batch_accs = [x['val_acc'] for x in outputs]\n",
    "        epoch_acc = torch.stack(batch_accs).mean()\n",
    "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
    "\n",
    "    def epoch_end(self, epoch, result):\n",
    "        print(\"Epoch [{}], val_loss: {:.4f}, val_acc: {:.4f}\".format(epoch, result['val_loss'], result['val_acc']))\n",
    "    \n",
    "model = RegModel()\n",
    "\n",
    "def evaluate(model, val_loader):\n",
    "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
    "    return model.validation_epoch_end(outputs)\n",
    "\n",
    "def fit(epochs, lr, model, train_loader, val_loader, opt_func=torch.optim.Adam):\n",
    "    history = []\n",
    "    optimizer = opt_func(model.parameters(), lr)\n",
    "    for epoch in range(epochs):\n",
    "        # Training Phase \n",
    "        for batch in train_loader:\n",
    "            loss = model.training_step(batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        # Validation phase\n",
    "        result = evaluate(model, val_loader)\n",
    "        model.epoch_end(epoch, result)\n",
    "        history.append(result)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'val_loss': 2.426126718521118, 'val_acc': 0.09948576241731644}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = evaluate(model, val_loader)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], val_loss: 0.2662, val_acc: 0.9227\n",
      "Epoch [1], val_loss: 0.2150, val_acc: 0.9389\n",
      "Epoch [2], val_loss: 0.2164, val_acc: 0.9390\n",
      "Epoch [3], val_loss: 0.2069, val_acc: 0.9416\n",
      "Epoch [4], val_loss: 0.2021, val_acc: 0.9442\n"
     ]
    }
   ],
   "source": [
    "history = fit(5, 0.05, model, train_loader, val_loader)\n",
    "# history2 = fit(10, 0.001, model, train_loader, val_loader)\n",
    "# history3 = fit(5, 0.0001, model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'val_loss': 0.17703935503959656, 'val_acc': 0.9541991949081421}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = MNIST(root='data/',train=False,transform=transforms.ToTensor())\n",
    "test_loader = DataLoader(test_dataset,batch_size=256)\n",
    "result = evaluate(model, test_loader)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(img, model):\n",
    "    xb = img.unsqueeze(0)\n",
    "    yb = model(xb)\n",
    "    _, preds = torch.max(yb, dim=1)\n",
    "    return preds[0].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 7 , Predicted: 7\n",
      "Label: 2 , Predicted: 2\n",
      "Label: 1 , Predicted: 1\n",
      "Label: 0 , Predicted: 0\n",
      "Label: 4 , Predicted: 4\n",
      "Label: 1 , Predicted: 1\n",
      "Label: 4 , Predicted: 4\n",
      "Label: 9 , Predicted: 9\n",
      "Label: 5 , Predicted: 5\n",
      "Label: 9 , Predicted: 9\n",
      "Label: 0 , Predicted: 0\n",
      "Label: 6 , Predicted: 6\n",
      "Label: 9 , Predicted: 9\n",
      "Label: 0 , Predicted: 0\n",
      "Label: 1 , Predicted: 1\n",
      "Label: 5 , Predicted: 5\n",
      "Label: 9 , Predicted: 9\n",
      "Label: 7 , Predicted: 7\n",
      "Label: 3 , Predicted: 3\n",
      "Label: 4 , Predicted: 4\n"
     ]
    }
   ],
   "source": [
    "model = model.eval()\n",
    "i=0\n",
    "count=0\n",
    "for x in test_dataset:\n",
    "    img, label = test_dataset[i]\n",
    "    if label == predict_image(img,model):\n",
    "        count+=1\n",
    "    print('Label:', label, ', Predicted:', predict_image(img, model))\n",
    "    i = i+1\n",
    "    if(i==20):\n",
    "        break\n",
    "# print(count/len(test_dataset))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
